{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T06:07:08.406689Z",
     "iopub.status.busy": "2025-04-25T06:07:08.406060Z",
     "iopub.status.idle": "2025-04-25T06:50:33.356605Z",
     "shell.execute_reply": "2025-04-25T06:50:33.355533Z",
     "shell.execute_reply.started": "2025-04-25T06:07:08.406659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# ðŸš€ SETUP --- global accuracy with precision, recall, and F1-score \n",
    "# ======================\n",
    "!pip install flwr --quiet\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import flwr as fl\n",
    "\n",
    "set_seed = lambda seed=42: [torch.manual_seed(seed), np.random.seed(seed), random.seed(seed)]\n",
    "set_seed()\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"/kaggle/input/data70k/\"\n",
    "GLOVE_PATH = os.path.join(DATASET_PATH, \"glove.6B.100d.txt\")\n",
    "\n",
    "# Global config\n",
    "MAX_WORDS = 3000\n",
    "SEQ_LEN = 100\n",
    "EMBED_DIM = 100\n",
    "\n",
    "# Tokenizer and encoder\n",
    "global_tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "global_label_encoder = LabelEncoder()\n",
    "fitted_tokenizer = False\n",
    "fitted_label_encoder = False\n",
    "\n",
    "# ======================\n",
    "# ðŸ“… Load Client Data\n",
    "# ======================\n",
    "def load_client_data(path):\n",
    "    global fitted_tokenizer\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    labels = df['target'].values\n",
    "\n",
    "    if not fitted_tokenizer:\n",
    "        global_tokenizer.fit_on_texts(texts)\n",
    "        fitted_tokenizer = True\n",
    "\n",
    "    sequences = global_tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=SEQ_LEN, padding='post')\n",
    "    X = torch.tensor(padded, dtype=torch.long)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)\n",
    "\n",
    "def load_test_data():\n",
    "    df = pd.read_csv(os.path.join(DATASET_PATH, \"test_data.csv\"))\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    texts = df[\"text\"].astype(str).tolist()\n",
    "    labels = df[\"target\"].values\n",
    "\n",
    "    sequences = global_tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=SEQ_LEN, padding='post')\n",
    "    X = torch.tensor(padded, dtype=torch.long)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return DataLoader(TensorDataset(X, y), batch_size=32)\n",
    "\n",
    "# ======================\n",
    "# ðŸ”  Load GloVe\n",
    "# ======================\n",
    "def load_glove_embeddings():\n",
    "    embeddings_index = {}\n",
    "    with open(GLOVE_PATH, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coeffs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coeffs\n",
    "\n",
    "    vocab_size = min(MAX_WORDS, len(global_tokenizer.word_index) + 1)\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBED_DIM))\n",
    "    for word, i in global_tokenizer.word_index.items():\n",
    "        if i < MAX_WORDS:\n",
    "            vec = embeddings_index.get(word)\n",
    "            if vec is not None:\n",
    "                embedding_matrix[i] = vec\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "# ======================\n",
    "# ðŸ§ Model with Attention\n",
    "# ======================\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
    "    def forward(self, x):\n",
    "        weights = torch.softmax(self.attn(x), dim=1)\n",
    "        return torch.sum(weights * x, dim=1)\n",
    "\n",
    "class CNN_BiGRU_Attn(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, padding=1)\n",
    "        self.bigru = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.attn = Attention(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        gru_out, _ = self.bigru(x)\n",
    "        x = self.attn(gru_out)\n",
    "        return self.fc(self.dropout(x))\n",
    "\n",
    "# ======================\n",
    "# ðŸŒ¸ Flower Client\n",
    "# ======================\n",
    "class SentimentClient(fl.client.NumPyClient):\n",
    "    def __init__(self, model, trainloader):\n",
    "        self.model = model\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = load_test_data()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    def get_parameters(self, config): return [val.cpu().numpy() for val in self.model.state_dict().values()]\n",
    "    def set_parameters(self, parameters):\n",
    "        state_dict = self.model.state_dict()\n",
    "        for k, v in zip(state_dict.keys(), parameters):\n",
    "            state_dict[k] = torch.tensor(v)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.train()\n",
    "        for x, y in self.trainloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(x)\n",
    "            loss = self.criterion(out, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.get_parameters({}), len(self.trainloader.dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.eval()\n",
    "        total, correct, total_loss = 0, 0, 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.testloader:\n",
    "                out = self.model(x)\n",
    "                loss = self.criterion(out, y)\n",
    "                total_loss += loss.item()\n",
    "                total += y.size(0)\n",
    "                correct += (out.argmax(1) == y).sum().item()\n",
    "                all_preds.extend(out.argmax(1).cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "        acc = 100 * correct / total\n",
    "        avg_loss = total_loss / len(self.testloader)\n",
    "        precision = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "        return avg_loss, total, {\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": precision * 100,\n",
    "            \"recall\": recall * 100,\n",
    "            \"f1_score\": f1 * 100\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# ðŸš€ Run FL Simulation\n",
    "# ======================\n",
    "from flwr.common import Context\n",
    "\n",
    "def client_fn(cid: str):\n",
    "    path = os.path.join(DATASET_PATH, f\"client_{int(cid)+1}_data.csv\")\n",
    "    trainloader = load_client_data(path)\n",
    "    model = CNN_BiGRU_Attn(embedding_matrix, hidden_dim=128, output_dim=2)\n",
    "    client = SentimentClient(model, trainloader)\n",
    "    return client.to_client()\n",
    "\n",
    "_ = load_client_data(os.path.join(DATASET_PATH, \"client_1_data.csv\"))\n",
    "embedding_matrix = load_glove_embeddings()\n",
    "\n",
    "def weighted_average(metrics):\n",
    "    total_examples = sum(num_examples for num_examples, _ in metrics)\n",
    "    def weighted(key):\n",
    "        return sum(num_examples * m[key] for num_examples, m in metrics) / total_examples\n",
    "    return {\n",
    "        \"accuracy\": weighted(\"accuracy\"),\n",
    "        \"precision\": weighted(\"precision\"),\n",
    "        \"recall\": weighted(\"recall\"),\n",
    "        \"f1_score\": weighted(\"f1_score\")\n",
    "    }\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")\n",
    "\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=3,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Global Metrics After Each Round:\")\n",
    "for round_num, acc in history.metrics_distributed[\"accuracy\"]:\n",
    "    prec = history.metrics_distributed[\"precision\"][round_num - 1][1]\n",
    "    rec = history.metrics_distributed[\"recall\"][round_num - 1][1]\n",
    "    f1 = history.metrics_distributed[\"f1_score\"][round_num - 1][1]\n",
    "    print(f\"Round {round_num} => Accuracy: {acc:.2f}%, Precision: {prec:.2f}%, Recall: {rec:.2f}%, F1-score: {f1:.2f}%\")\n",
    "\n",
    "final_acc = history.metrics_distributed[\"accuracy\"][-1][1]\n",
    "final_precision = history.metrics_distributed[\"precision\"][-1][1]\n",
    "final_recall = history.metrics_distributed[\"recall\"][-1][1]\n",
    "final_f1 = history.metrics_distributed[\"f1_score\"][-1][1]\n",
    "print(f\"\\nFinal Metrics â€” Accuracy: {final_acc:.2f}%, Precision: {final_precision:.2f}%, Recall: {final_recall:.2f}%, F1-score: {final_f1:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7125912,
     "sourceId": 11380834,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7125979,
     "sourceId": 11380918,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7126287,
     "sourceId": 11381295,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7234321,
     "sourceId": 11534488,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7234481,
     "sourceId": 11534801,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7234724,
     "sourceId": 11535332,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
